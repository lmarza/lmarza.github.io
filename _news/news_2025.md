---
layout: post
date: 2025-1-1
inline: true
---

&emsp;**July** 

- *"Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation"*  availble <a href='https://arxiv.org/pdf/2504.21643'>here!</a> has been accepted at IEEE RA-L 2025! ðŸ¤–

- New preprint *"Formal Verification of Variational Quantum Circuits"* is availble <a href='https://www.arxiv.org/pdf/2507.10635'>here</a>!

- *"Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation"* availble <a href='https://arxiv.org/pdf/2505.05235'>here</a> has been accepted at ECAI 2025! ðŸš€

- New preprint *"Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification"* is availble <a href='https://www.arxiv.org/pdf/2507.05405'>here</a>!


&emsp;**May** 

- <a href='https://arxiv.org/pdf/2407.01639'>ModelVerification.jl</a> the first Julia cutting-edge toolbox that contains a suite of state-of-the-art methods for verifying DNNs made during my research period abroad at CMU has been accepted at CAV 2025!ðŸ¤© The toolbox is available <a href='https://github.com/intelligent-control-lab/ModelVerification.jl'>here</a>. 

- New preprint *"Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation"* is availble <a href='https://arxiv.org/pdf/2505.05235'>here</a>!

- RobustX made in collaboration with Imperial College people has been accepted at IJCAI 2025!ðŸš€ We propose a novel Python library to generate robust counterfactual explanations! Check out the <a href='https://arxiv.org/pdf/2502.13751'>paper</a> and the <a href='https://github.com/RobustCounterfactualX/RobustX'>code</a>!


- Our paper *"Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation"*  is now available in preprint <a href='https://arxiv.org/pdf/2504.21643'>here!</a> We propose a hierarchical control framework leveraging neural network verification techniques to design control barrier functions (CBFs) and policy correction mechanisms that ensure safe reinforcement learning navigation policies.ðŸ¤–


&emsp;**February** 

- The journal extension of our paper *"Rigorous Probabilistic Guarantees for Robust Counterfactual Explanations"* in collaboration with <a href='https://fraleo.github.io'>Francesco Leofante</a> of <a href='https://www.imperial.ac.uk/explainable-artificial-intelligence/'> Centre for Explainable AI</a> is out! Read it <a href='https://www.researchgate.net/publication/389435880_Probabilistically_Robust_Counterfactual_Explanations_under_Model_Changes)'>here</a>ðŸš€ 

- RobustX is out!ðŸš€ A new Python library to generate robust counterfactual explanations! Check out the <a href='https://arxiv.org/pdf/2502.13751'>paper</a> and the <a href='https://github.com/RobustCounterfactualX/RobustX'>code</a> made in collaboration with Imperial College people!


&emsp;**January** 

- Our proposal on "Tackling Environmental Sustainability Challenges via Reinforcement Learning and Counterfactual Explanations" has been accepted for discussion at the AAAI 2025 Bridge on Explainable AI, Energy, and Critical Infrastructure Systems. Looking forward to engaging discussions on Explainable AI for environmental sustainability!

- Our paper <a href='https://arxiv.org/pdf/2406.08315'>Improving Policy Optimization via Îµ-Retrain</a> made during my research period at CMU under the supervision of <a href='https://www.ri.cmu.edu/ri-faculty/changliu-liu/'>Prof. Changliu Liu</a>, has been accepted at AAMAS 2025 ðŸ¤© Happy to have collaborated with <a href='https://emarche.github.io'>Enrico Marchesini</a> and <a href='https://priyadonti.com'>Prof. Priya Donti</a> of the Laboratory for Information & Decision Systems at MIT for this project!
 

